{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Heuristic_user_segmentation.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhd4HlVVovTf",
        "colab_type": "text"
      },
      "source": [
        "##Heuristic user segmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBEUNL8xohqj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-3.0.0-preview/spark-3.0.0-preview-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.0.0-preview-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnzHRhbNoi2_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-preview-bin-hadoop2.7\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SQLContext, SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, StringType\n",
        "from time import sleep\n",
        "from pyspark.streaming import StreamingContext\n",
        "#Spark Contexto\n",
        "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
        "from pyspark.sql import SparkSession\n",
        "#Spark Session\n",
        "ss = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Python Spark create REDIS HEMOCE\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .getOrCreate()\n",
        "    \n",
        "sc.version "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "006YOye7olWw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.streaming import StreamingContext\n",
        "\n",
        "# Você também precisa usar essas bibliotecas específicas\n",
        "from ua_parser import user_agent_parser\n",
        "from hyperloglog import HyperLogLog"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERYEvreFooUJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sc = SparkContext(master='local[4]')\n",
        "\n",
        "# Preparando lotes com os dados de entrada\n",
        "DATA_PATH = \"/data/course4/uid_ua_100k_splitted_by_5k\"\n",
        "batches = [sc.textFile(os.path.join(DATA_PATH, path)) for path in os.listdir(DATA_PATH)]\n",
        "\n",
        "# Criando Dstream para emular a geração de dados em tempo real\n",
        "BATCH_TIMEOUT = 5 # Tempo limite entre a geração do lote\n",
        "ssc = StreamingContext(sc, BATCH_TIMEOUT)\n",
        "dstream = ssc.queueStream(rdds=batches)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLx2e-Azww70",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "finished = False\n",
        "printed = False\n",
        "\n",
        "def set_ending_flag(rdd):\n",
        "    global finished\n",
        "    if rdd.isEmpty():\n",
        "        finished = True\n",
        "\n",
        "def print_only_at_the_end(rdd):\n",
        "    global printed\n",
        "    if finished and not printed:\n",
        "        # Digite seu código para classificar e imprimir o RDD resultante\n",
        "        \n",
        "        printed = True\n",
        "\n",
        "# Se tivermos recebido um vazio, o fluxo será concluído.\n",
        "# Então imprima o resultado e pare o contexto.\n",
        "\n",
        "dstream.foreachRDD(set_ending_flag)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqyv-9MLw5O3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ssc.checkpoint('./checkpoint{}'.format(time.strftime(\"%Y_%m_%d_%H_%M_%s\", time.gmtime())))         \n",
        "ssc.start()\n",
        "while not printed:\n",
        "    time.sleep(0.1)\n",
        "ssc.stop()  # quando o resultado for impresso, pare o contexto do Spark Streaming\n",
        "sc.stop()   # para o contexto do Spark para poder reiniciar o código sem reiniciar o kernel"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}