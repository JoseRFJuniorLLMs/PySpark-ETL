{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Contagem_de_palavras_com_estado.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMA2jcTDy2WF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-3.0.0-preview/spark-3.0.0-preview-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.0.0-preview-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oosvsVQmzGya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-preview-bin-hadoop2.7\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SQLContext, SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, StringType\n",
        "from time import sleep\n",
        "from pyspark.streaming import StreamingContext\n",
        "#Spark Contexto\n",
        "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
        "from pyspark.sql import SparkSession\n",
        "#Spark Session\n",
        "ss = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Python Spark create REDIS HEMOCE\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .getOrCreate()\n",
        "    \n",
        "sc.version "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3u5oPow7zU8v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.streaming import StreamingContext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRax-aduzX32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preparing SparkContext\n",
        "sc = SparkContext(master='local[4]')\n",
        "\n",
        "# Preparing batches with the input data\n",
        "DATA_PATH = \"/data/wiki/en_articles_streaming\"\n",
        "\n",
        "batches = [sc.textFile(os.path.join(DATA_PATH, path)) for path in os.listdir(DATA_PATH)]\n",
        "\n",
        "# Creating Dstream to emulate realtime data generating\n",
        "BATCH_TIMEOUT = 5  # Timeout between batch generation\n",
        "ssc = StreamingContext(sc, BATCH_TIMEOUT)\n",
        "dstream = ssc.queueStream(rdds=batches)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FT87qrLLzeaf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "finished = False\n",
        "printed = False\n",
        "\n",
        "\n",
        "def set_ending_flag(rdd):\n",
        "    global finished\n",
        "    if rdd.isEmpty():\n",
        "        finished = True\n",
        "\n",
        "\n",
        "def print_only_at_the_end(rdd):\n",
        "    global printed\n",
        "    # Type your code for printing the sorted data from the stream in a loop\n",
        "    \n",
        "    if finished and not printed:\n",
        "        # Type your code for printing the sorted data from the stream in a loop\n",
        "        \n",
        "        printed = True\n",
        "\n",
        "\n",
        "# If we have received an empty rdd, the stream is finished.\n",
        "# So print the result and stop the context.\n",
        "dstream.foreachRDD(set_ending_flag)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6IGVwNizqpP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ssc.checkpoint('./checkpoint{}'.format(time.strftime(\"%Y_%m_%d_%H_%M_%s\", time.gmtime())))  # checkpoint for storing current state        \n",
        "ssc.start()\n",
        "while not printed:\n",
        "    pass\n",
        "ssc.stop()  # when the result was printed, stop SparkStreaming context\n",
        "sc.stop()  # stop Spark context to be able restart the code without restarting the kernel"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}